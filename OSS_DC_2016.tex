%%%%%%%%%%%%%%%%%%%%%%%%%% author.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your contribution to an IFIP volume
% published at Springer
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%


% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[ifip]{svmult}

% choose options for [] as required from the list
% in the Reference Guide, Sect. 2.2
\usepackage{epsfig} 
\usepackage{makeidx}         % allows index generation
\usepackage{graphicx}        % standard LaTeX graphics tool
                             % when including figure files
\usepackage{multicol}        % used for the two-column index
\usepackage[bottom]{footmisc}% places footnotes at page bottom
\usepackage{float}
% etc.
% see the list of further useful packages
% in the Reference Guide, Sects. 2.3, 3.1-3.3

\makeindex             % used for the subject index
                       % please use the style sprmidx.sty with
                       % your makeindex program


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title*{Quantitative analysis of performance in key parameter of code review - Defects Individuation.}
%code review in software development in cloud-based system }
% Use \titlerunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
\author{Dorealda Dalipaj\inst{1}}
% Use \authorrunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
\institute{University Rey Juan Carlos, Madrid.
\texttt{dorealda.dalipaj@urjc.es}}
%
% Use the package "url.sty" to avoid
% problems with special characters
% used in your e-mail or web address
%
\maketitle

\textbf{\textit{Abstract -}} Finding and removing defects close to their point of injection remains the main motivation and a key parameter
to code review. However, different studies argued that  as many 
software programs rely on bug reports to correct software errors in maintenance activities, 
developers spend much time to identify these bug reports, the time taken to carry out the process of 
code review is long and the performance of code review in finding errors was less than expected.

The aim of this study is to bring empirical evidence on the time spent by developers to identify the bugs reports, the time
spent by them to carry out the reviewing process and 
to bring evidence by quantifying the performance of the very key parameter of code review: 
individuation of bugs.

With the abundance of data and having a diverse set of projects to observe, our case study is focused on the code review process of the 
cloud computing software, Open Stack. Our focus is to bring answers to what extent developers are actually individuating and fixing bugs 
while they are applying changes in the code finding thus if there is any code review providing more value than the others. 

The identification of the bugs report time, review time and the number of bugs fixed during this process, are the most 
fundamental parameters for characterization of the performance of the code review process and the most important metrics having 
a positive increasing relation with the benefits of code review.


%We focus our case study on a cloud computing software, Open Stack \cite{contribution11}. A developer can contribute to its source code 
%repository using the Launchpad Bugtracking \cite{contribution12} and 
%Gerrit Code Review \cite{contribution13}. First, from Launchpad, we individuate the bug reports that are actually describing a defect, 
%and extracted the needed information to bring statistical evidence on the number of defects that are individuated in Open Stack through the 
%years. Second, we compare the first results to the total number of issues reported, grouped by their different types, 
%to have proof of the rate at which the defects are discovered. And last we bring empirical evidence on the effort spent by developers, 
%by means of time, to carry out the defect individuation process.

%Code review is a well-established and cost-effective way to find defects. 
%Today code review is tool assisted, like from Bug tracking systems that were developed to guide maintenance activities of 
%software developers. Thus today it results time efficient and more lightweight compared to the early software inspections 
%performed in the 80s.  

%However, different studies (\cite{contribution1}, \cite{contribution9}, \cite{contribution20} ) had shown that code review 
%is not performing as expected. 
%They argued that the performance of code review 
%was quite low and that the actual outcome of code review in finding errors was less than expected. Furthermore, another study 
%\cite{contribution10}
%argues that, as many software programs 
%rely on bug reports to correct software errors in maintenance activities, developers spend much time to identify these bug reports. 

%Thus, the aim of this study is to bring evidence by quantifying the performance of the very key parameter of code review: individuation of bugs.
%We focus our case study on a cloud computing software, Open Stack \cite{contribution11}. 
%A developer can contribute to its source code repository using the Launchpad Bugtracking \cite{contribution12} and 
%Gerrit Code Review \cite{contribution13}. First, from Launchpad, we individuate the bug reports that are actually describing a defect, 
%and extracted the needed information to bring statistical evidence on the number of defects that are individuated in Open Stack through the 
%years. Second, we compare the first results to the total number of issues reported, grouped by their different types, 
%to have proof of the rate at which the defects are discovered. And last we bring empirical evidence on the effort spent by developers, 
%by means of time, to carry out the defect individuation process.

\section{Introduction}
\label{sec:1}
% Always give a unique label
% and use \ref{<label>} for cross-references
% and \cite{<label>} for bibliographic references
% use \sectionmark{}
% to alter or adjust the section heading in the running head

Code review, sometimes referred as peer review, employed both in industrial and open source contexts, 
is an activity in which people, other than the author of a software deliverable, 
examine it for defects and improvement opportunities.
Code review is characterized \textit{ as a systematic approach to examine a product in detail, 
using a predefined sequence of steps to determine if the product is fit for its intended use}
\cite{contribution14}.

There have been different ways of performing defect detenction since its beginning up to nowadays. The formal review or inspection 
according to Fanagan \cite{contribution15} approach required the conduction of an inspection meeting for actually finding of 
defects. Different controlled experiments showed that there were no significant differeneces in the total number of defects found 
when comparing meeting-based with meeting-less-based inspections \cite{contribution17}, \cite{contribution18}. Other 
studies \cite{contribution19} were carried out and proved that more defects were identified with meeting-less-based approaches. 
As a result a wide range of mechanisms and techniques of code review were developed. From static analysis 
\cite{contribution3} \cite{contribution4} \cite{contribution5}, which examines the code in 
the absence of input data and without running the code and is tool based, to modern code review 
\cite{contribution6} \cite{contribution7} \cite{contribution8}, which aligning with the distributed 
nature of many projects is asynchronous and frequently supporting geographically distributed reviewers. Because of their many uses 
and benefits, code reviews are a standard part of the modern software engineering workflow.

Although it is generally accepted that quality in software remains a challenge due to defects presence. A major quality issue with 
software is that defects are a byproduct of the complex development process and the ability to develop defect free software 
remains a big challenge for the software community. 
It is possible to improve the quality of software product by injecting fewer defects or by identifying and 
removing defects injected. It is also generally accepted that performance of software review is affected by several factors of the 
defect detection process. Code review performance is associated with the effort spent to carry out the process and the number of 
defects found. 

Most empirical studies try to assess the impact of specific process settings on performance.
Sources of process variability range from structure (how steps of the inspection 
are organized), inspection inputs (reviewer ability and product quality), 
techniques applied to defect identification that define how each step is carried out), 
context and tool support \cite{contribution16}.  
A controlled experiment by Johnson and Tjahjono \cite{contribution17} showed that \textit{total defects identified, effort spent in the 
process, false positive defects, duplicates} are fundamental variables to analyse when controlling the performance of code review. 

\section{Discussion}
\label{sec:2}

%Static analysis examines code in 
%the absence of input data and without running the code, and can
%detect potential security violations (e.g., SQL injection), runtime errors (e.g., dereferencing a null pointer) and logical
%inconsistencies (e.g., a conditional test that cannot possibly be true). A growing number of projects and companies are concerned about 
%individuating bugs in their code and they are making this process part of their standard build and testing system. 
%According to \cite{contribution2}, Google has incorporated FindBugs into their standard testing and code review process, 
%and has fixed more than 1,000 issues in their internal code base identified by the tool.
%More powerful tools \cite{contribution3}, \cite{contribution4}, \cite{contribution5}, evaluate software in the
%abstract, without executing them or considering a specific input. It has become fairly clear that static analysis
%tools can find important defects in software.

Although code review is used in software engineering primarily for finding defects, several studies argue that they often 
do not find functionality defects.

Over the past two years, a common tool for code review, Code Flow, has achieved wide-spread adoption at Microsoft. 
The functionality of CodeFlow is similar to other review tools such Mondrian \cite{contribution6} (adopted at Google), Phabricator 
\cite{contribution7} (adopted at Facebook) or open-source Gerrit \cite{contribution8}.
Two studies where conducted at Microsoft, with Code Flow as case study, on code review process.

The first study (\cite{contribution9}) took place with professional developers, testers, and managers at Microsoft. 
The results show that, although the top motivation driving code reviews is finding defects, the practice and the 
actual outcomes are less about finding errors than expected: Defect related comments comprise a small proportion, only 14\%, 
and mainly cover small logical low-level issues.
The second study conducted at Microsoft \cite{contribution20} stated that code review do not find bugs. They found that only about 
15\% of the comments provided by reviewers indicate a possible defect, much less functionallity issues that should block a 
code submission. 

Another empirical study on the effectiveness of security code review \cite{contribution1}, conducted an experiment on 30 developers. They 
conducted manual code review of a small web application. The web application supplied to the developers had seven known vulnerabilities. 
Their findings concluded that none of the subjects found all confirmed vulnerabilities (they were able to find only 5 out of 7) 
and that reports of false vulnerabilities were significantly correlated with reports of valid vulnerabilities.

A different experiment \cite{contribution10} argued that in large scale software programs where bug tracking systems 
are used,due to the excessive number of duplicate bug reports, developers spend much time to identify these bug reports. 

Keeping in mind the above discussion we did these questions:

RQ 1. What amount of time do developers need to identify bug reports?

RQ 2. What is the actual amount of time that developers take in carrying out the review process?

RQ 3. Are all the code review tools performing with a low number of defects detection?

With the abundance of data coming from the engineering systems and having a diverse set of projects to observe 
\cite{contribution12} \cite{contribution13}, we ask 
if there is any code review providing more value than the others?

To provide answer for the above question we perform a large empirical study on the actual 213 active projects of Open Stack.
For the purpose of our study we analyse them devided by the 9 projects that are the core of Open Stack (see \ref{sec:3}), and 
group the rest of them as Other Projects category.  
We choose Open Stack as it is a large project that have adopted code reviews on a large 
scale and have a reasonable traceability between commits, review and defects reports.
It uses Launchpad, a bugtracking system for tracking the issue reports, and Gerrit, a lightweight code review tool. Additionally, 
being an open source cloud computing software, it is backed by a global collaboration of developers. It has other flavors worthy of 
additional benefits which influences the outcome, and which can bring a different picture from the one in \cite{contribution1}, 
\cite{contribution9}, \cite{contribution20} and \cite{contribution10}. 

In the remainder of this paper, we first describe the necessary background notions for our work (section 3). Next, we describe the 
case study setup (section 4), then present the results of the research questions (section 5). After discussion and threats to 
validity (section 6 and section 7), we discuss future work (section 8) and we finish with conclusions (section 9).

\section{Background}
\label{sec:3}

This section provides background information about the bugtracking and code review environments of Open Stack and the 
tools for obtaining data from their repositories.

Openstack is a free and open source set of software tools for building and managing cloud computing platforms. 

%It is being employed by 263 users in different industries including: Web/ SaaS/ eCommerce, Information Technology, 
%Healthcare, Cloud Hosting/ MSP/ Telco, Film/ Media/ Gaming, Manifacturing/ Industrial, Retail, Finance ecc.

%The OpenStack Foundation, which oversees both development and community-building around the project, 
%serves more than 30,000 Individual Members from over 170 countries around the world. It is supported by 
%26 platinum and gold members (including AT\&T, Ubuntu, HPE, IBM, Intel, Red Hat, Rackspace, SUSE, Cisco, Dell, Ericson and many 
%5others), 3 infrastructure donors (Rackspace, HPE, OVH Group), and 550 corporate donors and supporting organizations.
%Backed by some of the biggest companies in software development and hosting, as well as thousands of individual community members, 
%OpenStack is the future of cloud computing. 

OpenStack is made up of many different moving parts. Because of its open nature, anyone can add additional components to OpenStack 
to help it to meet their needs. This is whay actually in Open Stack there are 213 active projects. 
But the OpenStack community has collaboratively identified 9 key components that are a part of 
the "core" of OpenStack, which are distributed as a part of any OpenStack system and officially maintained by the OpenStack community: 
Nova, Swift, Cinder, Neutron, Horizon, Keystone, Glance, Ceilometer, and Heat.
Thus in this study we analyse the 9 core components of Open Stack and categorise the rest as Other Projects.

Open Stack uses Launchpad, an issue tracking system, which is a repository used to enable
users and developers to report defects and feature requests. It allows such a reported issue to be triaged and (if  deemed
important) assigned to team members, to discuss the issue
with any interested team member and to track the history of
all work on the issue. During these issue discussions, team
members can ask questions, share their opinions and help
other team members. Open Stack uses a dedicated reviewing enivornment, 
Gerrit, to  review  patches  and  bug  fixes. It supports with 
lightweight processes for reviewing code changes, i.e., 
to decide whether a developer’s change is safe to
integrate into the official Version Control System. During such
lightweight  code  review,  assigned  reviewers  make  comments
on a code change or ask questions that can lead to a discussion
of  the  change  and/or  different  revisions  of  the  code  change,
before  a  final  decision  is  made  about  the  code  change.  If
accepted, the most recent revision of the code change can enter
the version control system, otherwise the change is abandoned
and the developer will move on to something else.  

In this paper, we are interested in quantifying the time that developers need 
to go through the code review, by measuring the time to merge a change,
and to measure how many bugs (and possibly of what type) where fixed in the
code changes that successfully  passed in the VCS.

%A developer can contribute to an OpenStack source code repository using Gerrit code review system. Since the Web interface for 
%Gerrit uses Launchpad OpenID single sign-on to identify a developer, an account on bug tracking platform Launchpad is necessary. In 
%addition to the issue and code review repositories, Open Stack adopted modern technology of git for version control system. 
%Furthermore Open Stack is among pioneers for pushing developers to explicitly link code changes to issues and reviews.

To obtain the issue reports and code review data of these ecosystems, we used the data set 
provided by Gonzalez-Barahona et al. \cite{contribution21}.  They  developed the  MetricsGrimoire tool suite to mine the repositories of
OpenStack, then store the corresponding data into a relational database. We make use of their issue
report and code review data sets \cite{contribution22} to perform our study.


\section{Case Study Setup}
\label{sec:4}

This section explains the methodology used to address our research questions.  
We discuss the selection of the case study system, how we identified and the extraction of 
bug reports, and the results that we obtained.


\subsection{Selection of Case Study System}
The aim of our study, to begin with, is to measure the time that the developers need to identify 
the bug reports in the bugtracking system, 
the time they need to carry out the review, and provide quantitative evidence on these two metrics and
on the number of bugs detected during the process. 
\\
\\
Our case study system choice falls on Open Stack, because for achieving our aims, we required projects with a
substantial number of commits linked to issue reports and code review. And this is 
readily done in Open Stack.

Furthermore, thanks to MetricsGrimoire tool, we can mine the repositories of Launchpad and Gerrit systematically updated.

What we need to do is to identify the issue reports classified as bugs, link them the respective review and then 
extract the patterns we need for carring out our results.

\subsection{Identifying classified Bug Reports}
\label{sec:4.1}

In Launchpad, besides bugs reports, the developers can work with \textit{specifications} 
(approved design specifications for additions and changes to the project team’s code repositories) 
and \textit{blueprints} (lightweight feature specifications).

Identify which of the reports have been classified as describing bugs is not a trivial task.
Tickets usually are commented. Reviewers do discuss about having found a bug in a certain report or not. 
But we saw that analysing comments of a ticket is not the most efficient way for extracting its classification, 
because we wil not recover 100\% of the tickets.
\\
\\
Manually analysing the tickets and how the Launchpad work flow treats them, we came accross a pattern 
of the reports states with regards to confirming new bugs:

1. a ticket, stating a possible bug, is opened in launchpad and its status is New,

2. if the problem stated in the ticket is reproduced than the bug is confirmed as genuine and the 
ticket status passes from New to Confirmed,

3. only when a bug is confirmed, the status then changes from Confirmed to In Progress and 
an issue will be opened in gerrit.
\\
\\
Thus, we automaticlly analysed the Launchpad repository searching for tickets that have a match with this pattern. 
These are the tickets that have been classified as bug reports. Once we identify we extract them in a new repository for further 
inspection.   
\\
\\
Our results showed that \/ 57720 tickets out of 88421 have been classified as bugs 
(you can review this results in a python notebook 
\footnote{github.com/ddalipaj/CR\_Defect\_Individuation\_Rate/blob/master/finding\_bugs.ipynb} 
that we make availabe online). 
Hence 65.3\% of the total tickets in Launchpad are bugs, and an issue for fixing has been opened for them in Gerrit. 
At this point we were able to quantify the time that developers spend on identifying bug reports as the distance between 
the moment when the ticket is first inserted in Launchpad up to the moment it is confirmed as a genuine bug.

\begin{figure}[H]
\centering
 The percentages of reported issues (tickets) classified as bugs in OpenStack, grouped by projects and last the percentage 
 on the total number of tickets.
\includegraphics[width=1.0\textwidth,natwidth=786,natheight=360]{bugs_time.png}

\caption{The percentages of reported bugs in OS - From July, 2010 - January, 2016.}
\label{fig:1}       % Give a unique label
\end{figure}


\subsection{Linking the Issue Reports to the Reviews}
\label{sec:4.2}

The next step is to link the tickets that we have already extracted with their respective issue in the code review system.
To detect the links between ticket and reviews, we first referred to the name of the branch on which a code change had 
been made, since some of them follow the naming convention "bug/989868" with "989868" a ticket identifier.
After the extraction, we manually analysed a random number of issues and their respective tickets. We discovered 
that some issues were matched to a ticket like a related artifact, while indeed the issue was reviewing the ticket and merging 
not in the master branch of the project under which the defect was originated, but in another branch. 
For quatifying the time that developers do need to carry out the review process, we must take into consideration only the 
merges into the master branch of the project, the merges in the versions of the same project for the preservation of compatibility 
are not elements of this metric. Thus this selection was clearly erroneus.
We tried the other way around approach. Instead of linking reviews to tickets, we linked tickets to reviews using the information
that we find in the comments of the tickets. Whenever a review recieves a proposal for a fix, or a merge for a fix, it is reported in the 
comments of the respective ticket. 
Precisely, a merge comment looks like the following:  
\\
\\
Reviewed:  https://review.openstack.org/100018

Committed: https://git.openstack.org/cgit/openstack/nova/commit
\\
/?id=be58dd8432a8d12484f5553d79a02e720e2c0435

Submitter: Jenkins

Branch:    master ...
\\
\\
Where the first line, clearly, provide us with the link to the issue in Gerrit. 

The first problem that arises in analysing the comments is that, some of them for some ticket, are a summary of some commit history. 
Therefore, in these cases, we find more than a match with the pattern we are looking for within the body of 
the comment, while the commit itself is not a merge in the master branch of the project that orginated the ticket, 
consequently not the correct result. 

However there is a fixed format of the comments that report a merge, which is the one in the example above.
The information related to the review is stated at the very begining of the 
comment and manually analysing the tickets in Launchpad we have seen that they are found in the first 6 rows.

Thus the first step is trunking of the comments, so that we extract just the first 6 lines from every one of them. 
Doing this assures the identifying of the right review. We can view the steps of this process in a python notebook
\footnote{https://github.com/ddalipaj/Analysis\_Tickets\_Issues/blob/master/master\_merge.ipynb}
that is available online. The table below shows the number of tickets and issues that we were able to link with its 
counterpart.

\begin{figure}[H]
\centering

\includegraphics[width=1.0\textwidth,natwidth=778,natheight=326]{t-i.png}

\caption{The percentages of tickets and issues linked with its counterparts in OS - From July, 2010 - January, 2016.}
\label{fig:2}       % Give a unique label
\end{figure}

At this point we are able to carry out the time to review in OpenStack as the distance between the first patch is 
uploaded to Gerrit up to when a fix change is merged to the code base.

\section{Case Study Preliminary Results}
\label{sec:5}

RQ 1. What amount of time do developers need to identify bug reports?

We computed the time to identify bug reports as discussed in \ref{sec:4.1}. 
Then, we calculated the median effect size across all Open Stack projects in order to globally rank
the metrics from most extreme effect size, and last the quantiles. The results are shown in the table 
below:

\begin{figure}[H]
\centering

\includegraphics[width=1.0\textwidth,natwidth=779,natheight=129]{t-c.png}

\caption{The median time to classify a bug report accross all projects in Open Stack - From July, 2010 - January, 2016.}
\label{fig:3}       % Give a unique label
\end{figure}

RQ 2. What is the actual amount of time that developers take in carrying out the review process?

We computed the time to carry out the review process as discussed in \ref{sec:4.2}.
Again, we calculated the median accross all Open Stack projects. The results are shown in the table below:

\begin{figure}[H]
\centering

\includegraphics[width=1.0\textwidth,natwidth=779,natheight=129]{t-r1.png}

\caption{The median time to classify a bug report accross all projects in Open Stack - From July, 2010 - January, 2016.}
\label{fig:4}       % Give a unique label
\end{figure}

RQ 3. Are all the code review tools performing with a low number of defects detection?

This is a preliminary step in the phd. The third research query is the topic of the next future, thus work in progress.
We can state that currently we are working with the elements of the review process that we dispose, that are 
the comments and the commit 
analysing both the human discussion and the changes in the code. 

\section{Threats to Validity}
\label{sec:6}

Threats to internal validity concern confounding factors that might influence the results. There
are likely unknown factors that impact defect-detection that we have not measured yet.
Due to the elaborate filtering that we performed in order to link two repositories (bug repository, 
and code review), the  heuristics  used  to  find  the
links  between  the  repositories  are  not  100\%  accurate,
however  we  used  the  state-of-the-practice  linking  algorithms
at  our  disposal.  Recent  features  in  Gerrit  show  that  clean
traceability  between  version  control  and  review  repositories
is now within reach of each project, hence the available data
for future of this study will only grow in volume.

\section{Future Work}
\label{sec:7} 

Our immediatly future work is to quantify the rate at which bugs are discovered during the code review process. 
In our study we focus on the time for identifying bug reports, for carrying out the review process and the number
of defects identified during review. But there are several other metrics that influences and characterize the 
performance of code review process that we would like to investigate.

\section{Conclusion}
\label{sec:8} 

In this paper we emprirically studied the impact of the time for identifying the bug reports, the time to carry out
the review process and are conducting a study on quantifying the number of bugs fixed during a code review. 
From the preliminary results that we bring into evidence and the future results that we hope to have, we believe that 
our study will open up a variety of research opportunities to continue investigating the impact of collaborative 
characteristics on performance assurance.   

\section{Aknowledgement}

This study is funded under SENECA EID project, funded under Marie-Skłodowska Curie Actions.
\\
\\
\\
The preliminary results of this study can be found in three python notebook available online:
\\
1. https://github.com/ddalipaj/CR\_Defects\_Individuation\_Rate/blob/master/
\\
finding\_bugs.ipynb
\\
2. https://github.com/ddalipaj/Analysis\_Tickets\_Issues/blob/master/
\\
master\_merge.ipynb
\\
3. https://github.com/ddalipaj/Reviewing\_Time\_Gerrit/blob/master/
\\
reviewing\_time\_Gerrit.ipynb
\\
\pagebreak














%\subsection{Subsection Heading}
%\label{sec:3}
%Your text goes here.

%\begin{equation}
%\vec{a}\times\vec{b}=\vec{c}
%\end{equation}

%\subsubsection{Subsubsection Heading}
%See Sect.~\ref{sec:1}.

%\paragraph{Paragraph Heading} %
%Your text goes here.

%\subparagraph{Subparagraph Heading.} Your text goes here.%
%
%\index{paragraph}
% Use the \index{} command to code your index words
%
% For tables use
%
%\begin{table}
%\centering
%\caption{Please write your table caption here}
%\label{tab:1}       % Give a unique label
%
% For LaTeX tables use
%
%\begin{tabular}{lll}
%\hline\noalign{\smallskip}
%first & second & third  \\
%\noalign{\smallskip}\hline\noalign{\smallskip}
%number & number & number \\
%number & number & number \\
%\noalign{\smallskip}\hline
%\end{tabular}
%\end{table}
%
%
% For figures use
%
%\begin{figure}
%\centering
% Use the relevant command for your figure-insertion program
% to insert the figure file.
% For example, with the option graphicx use
%\includegraphics[height=4cm]{figure.eps}
%
%\caption{Please write your figure caption here}
%\label{fig:1}       % Give a unique label
%\end{figure}
%
% For built-in environments use
%
%\begin{theorem}
%Theorem text\footnote{Footnote} goes here.
%\end{theorem}
%
% or
%
%\begin{lemma}
%Lemma text goes here.
%\end{lemma}
%
%
% BibTeX users please use
% \bibliographystyle{}
% \bibliography{}
%
% Non-BibTeX users please follow the syntax
% the syntax of "referenc.tex" for your own citations
\input{referenc}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\printindex
\end{document}





